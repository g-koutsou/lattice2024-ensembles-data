% Please make sure you insert your
% data according to the instructions in PoSauthmanual.pdf
\documentclass[a4paper,11pt]{article}
\usepackage{pos}
\usepackage{lineno}[pagewise]
\usepackage{forloop}
\usepackage[para]{footmisc}

\NewDocumentCommand\Nf{mggg}{$N_{\textrm{f}}\text{=}#1\IfNoValueTF{#2}{}{\text{+}#2}\IfNoValueTF{#3}{}{\text{+}#3}\IfNoValueTF{#4}{}{\text{+}#4}$}
\NewDocumentCommand\Nas{mggg}{$N_{\textrm{as}}\text{=}#1\IfNoValueTF{#2}{}{\text{+}#2}\IfNoValueTF{#3}{}{\text{+}#3}\IfNoValueTF{#4}{}{\text{+}#4}$}
\NewDocumentCommand\Nadj{mggg}{$N_{\textrm{adj}}\text{=}#1\IfNoValueTF{#2}{}{\text{+}#2}\IfNoValueTF{#3}{}{\text{+}#3}\IfNoValueTF{#4}{}{\text{+}#4}$}
\NewDocumentCommand\vol{mg}{#1\textsuperscript{3}\IfNoValueTF{#2}{}{$\times$#2}}

\newcounter{linecounter}
\newenvironment{emptylines}[1]{
  \begin{linenumbers}[1]
    \forloop{linecounter}{1}{\value{linecounter} < #1}{
      \quad\\
    }
  \end{linenumbers}
}{}

\title{Lattice gauge ensembles and data management}

\author[a,\#]{Y.~Aoki}       \affiliation[a]{RIKEN Center for Computational Science (R-CCS), Kobe, 650-0047, Japan}
\author*[b,1]{E.~Bennett}     \affiliation[b]{Swansea Academy of Advanced Computing, Swansea University, Swansea, United Kingdom}
\author*[c,2]{R.~Bignell}     \affiliation[c]{School of Mathematics, Trinity College, Dublin, Ireland}
\author*[d,3]{K.~U.~Can}      \affiliation[d]{CSSM, Department of Physics, The University of Adelaide, Australia}
\author*[e,4]{T.~Doi}         \affiliation[e]{Interdisciplinary Theoretical and Mathematical Sciences Program (iTHEMS), RIKEN, Japan}
\author*[f,5]{S.~Gottlieb}    \affiliation[f]{Department of Physics, Indiana University, IN, USA}
\author*[g,6]{R.~Gupta}       \affiliation[g]{Theoretical Division, Los Alamos National Laboratory, NM, USA}
\author*[h,7]{G.~von Hippel}  \affiliation[h]{PRISMA+ Cluster of Excellence and Institut f\"ur Kernphysik}
\author*[a,8]{I.~Kanamori}   
\author*[i,9]{A.~Kotov}       \affiliation[i]{J\"ulich Supercomputing Center, Forschungszentrum J\"ulich, Germany}
\author*[j,10,\#]{G.~Koutsou} \affiliation[j]{Computation-based Science and Technology Research Center, The Cyprus Institute, Cyprus}
\author*[k,11]{R.~Mawhinney}  \affiliation[k]{Physics Department, Columbia University, USA}
\author*[l,12]{A.~Patella}    \affiliation[l]{Humboldt Universit\"at zu Berlin, Institut f\"ur Physik \& IRIS, Germany}
\author*[i,13]{G.~Pederiva}   
\author*[m,14]{C.~Schmidt}    \affiliation[m]{Fakult\"at f\"ur Physik, Universit\"at Bielefeld, Germany}
\author*[n,15]{T.~Yamazaki}   \affiliation[n]{Institute of Pure and Applied Sciences, University of Tsukuba, Japan}
\author*[o,16]{Y.-B.~Yang}    \affiliation[o]{School of Physical Sciences, University of Chinese Academy of Sciences, Beijing, China}

\note[1]{For the TELOS collaboration}
\note[2]{For the FASTSUM collaboration}
\note[3]{For the CSSM/QCDSF/UKQCD collaboration}
\note[4]{For the HAL QCD collaboration}
\note[5]{For the MILC collaboration}
\note[6]{For the Jlab/W\&M/LANL/MIT/Marseille effort}
\note[7]{For the CLS}
\note[8]{For the JLQCD collaboration}
\note[9]{For the TWEXT collaboration}
\note[10]{For the ETM collaboration (ETMC)}
\note[11]{For the RBC-UKQCD collaboration}
\note[12]{For the RC* collaboration}
\note[13]{For the OPEN LAT initiative}
\note[14]{For the HotQCD collaboration}
\note[15]{For the PACS collaboration}
\note[16]{For the CLQCD collaboration}

\note[\#]{Conveners}

%\emailAdd{g.koutsou@cyi.ac.cy}

\abstract{We summarize the status of lattice QCD ensemble generation
  efforts and their data management characteristics. Namely, this
  proceeding summarizes contributions to a dedicated parallel session
  during the 41\textsuperscript{st} International Symposium on Lattice
  Field Theory (Lattice 2024), during which representatives of 16
  lattice QCD collaborations provided details on their simulation
  program, with focus on plans for publication, data management, and
  storage requirements. The parallel session was organized by the
  International Lattice Data Grid (ILDG), following an open call to
  the lattice QCD for participation in the session.}


\FullConference{The 41st International Symposium on Lattice Field
  Theory (LATTICE2024)\\ 28 July - 3 August 2024\\ Liverpool, UK\\}

\tableofcontents

\begin{document}
\maketitle

\section{Introduction}
The simulation of Quantum Chromodynamics (QCD) via its Eucledean-time,
discrete formulation on a lattice, has been one of the most
compute-intensive applications in scientific computing, consuming
substantial fractions of computer time at leadership HPC facilities
internationally. In particular, the generation of ensembles of gauge
configurations, for multiple values of the QCD parameters such as the
QCD coupling, the quark masses, and the extent of the finite volume,
requires multi-year simulation campaigns, coordinated by multi-member
research collaborations. It is thus common that collaborations store
and reuse the same gauge ensembles for multiple observables of
interest, and in many cases also share the ensembles with researchers
external to the collaboration that generated them.

The purpose of this proceeding is to summarize the available gauge
ensembles generated by various lattice QCD collaborations
internationally, with a focus on the data management practices each
collaboration employs. It follows a parallel session at the
41\textsuperscript{st} International Symposium on Lattice Field Theory
(Lattice 2024), during which 16 collaborations provided status reports
of their simulation efforts, responding to an open call for
participation addressed to the lattice QCD community prior to the
conference. The first such session was during Lattice 2022 and a
report of the contributions presented during that session can be found
in Ref.~\cite{Bali:2022mlg}.

These sessions are organized by the International Lattice Data Grid
(ILDG) with the intention of obtaining gathering and summarizing the
evolving needs of the lattice community in terms of data storage and
management.  The ILDG was setup in the early
2000s~\cite{Davies:2002mu,Yoshie:2008aw,Maynard:2009szr,Beckett:2009cb}
by the lattice community, which realized early on the value in
standardizing data management practices across the field. ILDG is
organized as a federation of autonomous \textit{regional grids},
within a single Virtual Organization~\cite{ildg-organization}. It
standardizes interfaces for the services, which are to be operated by
each regional grid, such as storage and a searchable metadata catalog,
so that the regional services are interoperable. Within ILDG, working
groups specify community-wide agreed metadata schemas
(QCDml)~\cite{Coddington:2007gz} to concisely mark-up the gauge
configurations and develop relevant middleware tools for facilitating
the use of ILDG services. The middleware and metadata specifications
developed by ILDG adhere to most of the FAIR (Findable, Accessible,
Interoperable, Reusable) principles~\cite{Wilkinson2016}. A summary of
recent developments in ILDG, referred to as ILDG 2.0, was presented
during the same session and can be found in a separate
proceeding~\cite{ILDG2}.

In the remainder of this proceeding, we present the status of ensemble
generation of each of the 16 collaborations that contributed to the
parallel session. We restrict to simulations of QCD, and at present
these are carried out using \Nf{2}{1}, \Nf{2}{1}{1}, and
\Nf{1}{1}{1}{1} sea quark flavors with various fermion
discretizations. The contributors were asked to specify whether their
data are public or if they plan in making them public, their interest
in using ILDG services and tools for that purpose, as well as some
overall information regarding storage requirements. This information
is collected in a table and summary section that follows the
individual contributions.

\section{Contributions}
The contributions from each collaboration follow, in the order
presented during the parallel session. The original presentations can
be found on the conference website~\cite{parallel-session}.

\newpage
\subsection{CLQCD}
\emptylines{19}

\subsection{Jlab/W\&M/LANL/MIT/Marseille}
\emptylines{19}

\subsection{HotQCD}
\emptylines{19}

\subsection{FASTSUM}
\emptylines{19}

\subsection{TELOS}

The TELOS collaboration performs
\textbf{T}heoretical \textbf{E}xplorations on the \textbf{L}attice
with \textbf{O}rthogonal and \textbf{S}ymplectic groups.
Problems of interest focus on physics beyond the Standard Model,
in particular composite Higgs models.
Our work to date has made use of
the Wilson gauge action and Wilson fermion action.
Our ensembles include studies of
the Sp(4) theory with two fundamental fermion flavours
(\Nf{2})~\cite{Bennett:2019jzz}
(five values of $\beta\in[6.9,7.5]$,
$V \le 48\times42^{3}$,
$m_{\mathrm{PS}}/m_{\mathrm{V}} \gtrsim 0.407(16)$),
the Sp(4) theory with three antisymmetric fermion flavours
(\Nas{3})~\cite{Hsiao:2022gju},
(six values of $\beta\in[6.6,6.9]$,
$V \le 54\times36^{3}$,
$m_{\mathrm{ps}}/m_{\mathrm{v}} \gtrsim 0.7954(44)$);
and the Sp(4) theory with \Nf{2} and \Nas{3}~\cite{Bennett:2022yfa},
(three values of $\beta\in[6.45,6.5]$,
$V \le 56\times36^{3}$,
$m_{\mathrm{PS}}/m_{\mathrm{V}} \gtrsim 0.8768(30)$,
$m_{\mathrm{ps}}/m_{\mathrm{v}} \gtrsim 0.9022(27)$).
In the latter two cases,
the topological charge becomes slow running at small $m_{\mathrm{as}}$,
and at larger $\beta$.
We do not retain our pure gauge ensembles,
used for studies of the large-$N$ limit of Sp(2$N$)~\cite{Bennett:2023qwx,Bennett:2022gdz,Bennett:2020qtj},
since the costs of storage and data transfer are
higher than those of regenerating the ensembles.

Additionally,
we present ensembles generated by a subset of the collaboration,
with applications to conformal and near-conformal dynamics,
and to potential Walking Technicolor theories,
again using the Wilson gauge and Wilson fermion actions~\cite{Athenodorou:2024rba}.
Specifically,
these are SU(2) with one adjoint flavour (\Nadj{1})
(seven values of $\beta\in[2.05,2.4]$,
$V \le 96\times48^{3}$,
$m_{2^{+}_{s}} \gtrsim 0.28$),
and \Nadj{2}
($V \le 128\times64^{3}$,
$m_{2^{+}_{s}} \gtrsim 0.47$).
In the former case,
the majority of ensembles show ergodic topology,
with $\beta=2.4$ being marginal;
in the latter,
at large volumes we see significant topological freezing.

Ensembles are generated using
HiRep~\cite{Bennett:2019cxd,DelDebbio:2008zf}
and Grid~\cite{Bennett:2023gbe,Yamaguchi:2022feu}.
The above ensembles will be made available
as soon as the infrastructure is in place to do so.

We are in the process of generating ensembles
for Sp(4) \Nf{2}
and for SU(2) \Nf{1,2}
with Möbius domain wall fermions,
and for SU(2) \Nf{1,2}
with Wilson fermions
and additional Pauli--Villars fields,
which we aim to make available concurrently with the corresponding papers.



\subsection{HAL QCD}
\emptylines{19}

\subsection{TWEXT}
\begin{linenumbers}[1]
The TWEXT (Twisted Wilson @ Extreme conditions) collaboration studies
the properties of QCD at high temperature using Wilson Twisted Mass
fermions. Problems under investigation include chiral properties of
QCD, in particular the behavior of QCD around the chiral phase
transition and its scaling window~\cite{Kotov:2021rah}, topological
properties of QCD and QCD axion~\cite{Kotov:2021ujj}, hadron masses,
symmetries of QCD and others. For this purpose, TWEXT generated a set
of configurations for \Nf{2}{1}{1} fermions at the physical pion mass
and also uses older configurations with heavier pion
mass. Configurations with the physical pion mass have three lattice
spacings $a\in(0.057,0.080)$~fm and cover a wide range of temperatures
from $\sim120$ MeV to $\sim900$ MeV. It allows the TWEXT collaboration
to perform the continuum extrapolation for quantities of interest in
this temperature range. For the generation the tmLQCD software
package~\cite{Jansen:2009xp,Deuzeman:2013xaa,Abdel-Rehim:2013wba} is
used and the parameters of the ensembles were taken from the zero
temperature simulations of the ETM
collaboration~\cite{Alexandrou:2018egz}. Currently, the TWEXT
collaboration has 80 ensembles (one ensemble corresponds to one point
in the space temperature-pion mass-lattice spacing), which occupy
$\sim80$ TB of disk space. Configurations are stored in the ILDG
format. Possible collaborations are welcome and TWEXT plans to make
configurations public/use ILDG in the future, after performing the
ongoing analysis.
\end{linenumbers}

\subsection{CSSM/QCDSF/UKQCD}
\emptylines{19}

\subsection{RBC-UKQCD}
\emptylines{19}

\subsection{OPEN LAT}
\emptylines{19}

\subsection{RC*}
\emptylines{19}

\subsection{ETMC}
\begin{linenumbers}[1]
The ETM collaboration focuses on hadron spectroscopy, hadron
structure, and flavor physics at zero temperature. Ensembles employ
the twisted mass formulation, realizing $\mathcal{O}(a)$-improvement
by tuning to maximal twist, and include a clover term to further
reduce the size of lattice artifacts. The Iwasaki gauge action is
used. The main simulation effort is for the generation of ensembles
with degenerate up- and down-, strange- and charm-quarks
(\Nf{2}{1}{1}) with lattice spacing ranging between $0.049$ and
$0.091$~fm. $M_\pi\cdot L$ varies from $2.5$ up to ${\sim}5.5$. At the
time of writing, 24 ensembles are available or in the process of being
generated, with 8 of these at approximately physical values of the
quark masses. For a recent listing of the ensembles,
see~\cite{ETMCPoster:2024}. Simulations are performed using the Hybrid
Monte Carlo (HMC) algorithm implemented in the tmLQCD software
package~\cite{Jansen:2009xp,Deuzeman:2013xaa,Abdel-Rehim:2013wba}. See
Ref.~\cite{Alexandrou:2018egz} for details on the simulation program,
including the parameter tuning. The
DD-$\alpha$AMG~\cite{Frommer:2013fsa,Alexandrou:2016izb} multigrid
iterative solver is employed for the most poorly conditioned monomials
in the light sector while mixed-precision CG is used
elsewhere. Multi-shift CG is used together with shift-by-shift
refinement using DD-$\alpha$AMG~\cite{Alexandrou:2018wiv} for a number
of small shifts for the heavy sector. tmLQCD has interfaces to
QPhiX~\cite{Joo:2013lwm} and QUDA~\cite{Clark:2009wm,Babich:2011np}.
tmLQCD automatically writes gauge configurations in the ILDG format,
with meta-data including creation date, target simulation parameters,
and the plaquette. ETMC policy is to make ensembles publicly
available after a grace period. Older \Nf{2} and \Nf{2}{1}{1}
ensembles~\cite{Baron:2010bv,EuropeanTwistedMass:2010voq,ETM:2009ztk}
have made use of ILDG storage elements. The current ensembles are
available upon request and the collaboration intends to use ILDG in
the near future. For these ensembles, we expect storage requirements
to reach 3~PB.
\end{linenumbers}

\subsection{JLQCD}
\emptylines{19}

\subsection{MILC}
\emptylines{19}

\subsection{CLS}
\emptylines{19}

\subsection{PACS}
\emptylines{19}

\section{Summary}

\begin{table}[h]
  \caption{ Public: (2 = currently public, 1 = after an embargo period, 0 = no); ILDG: (N = no interest, I =
    interest, P = planned, U = already using); \#end: Number of
    ensembles; \#cfg: Total number of configurations; storage: Total
    storage needed in TBytes.
    \label{tab:summary}
  }
  \centering
  \begin{tabular}{lccrrr}\hline\hline
    Collaboration                & Public & ILDG & \#ens & \#cfg & Storage (TB) \\\hline
    CLQCD                        &        &      &       &       &              \\
    Jlab/W\&M/LANL/MIT/Marseille &        &      &       &       &              \\
    HotQCD                       &        &      &       &       &              \\
    FASTSUM                      &        &      &       &       &              \\
    TELOS                        &    1   &  P   &  250  & 800,000 & 120            \\
    HAL QCD                      &        &      &       &       &              \\
    TWEXT                        &    1   &  P   &  80   &70,000 &  80          \\
    CSSM/QCDSF/UKQCD             &        &      &       &       &              \\
    RBC-UKQCD                    &        &      &       &       &              \\
    OPEN LAT                     &        &      &       &       &              \\
    RC*                          &        &      &       &       &              \\
    ETMC                         &    1   &  P,U &  24   &12,000 & 3,000        \\
    JLQCD                        &        &      &       &       &              \\
    MILC                         &        &      &       &       &              \\
    CLS                          &        &      &       &       &              \\
    PACS                         &        &      &       &       &              \\\hline\hline
  \end{tabular}
\end{table}


\acknowledgments GK acknowledges support from EXCELLENCE/0421/0195,
co-financed by the European Regional Development Fund and the Republic
of Cyprus through the Research and Innovation Foundation and the
AQTIVATE a Marie Sk\l{}odowska-Curie Doctoral Network
GA~No.~101072344.
The work of EB is part-funded by
the UKRI Science and Technology Facilities Council (STFC)
Research Software Engineer Fellowship EP/V052489/1,
the UKRI Engineering and Physical Science Research Council (EPSRC)
ExCALIBUR programme ExaTEPP project EP/X017168/1,
and the STFC Consolidated Grant ST/T000813/1.

The participating collaborations acknowledge the following HPC
systems, for the generation of the gauge ensembles reported here,
LUMI-C and LUMI-G at CSC in Finland,
JUWELS and JUWELS-Booster at
Jülich Supercomputing Centre (JSC),
SuperMUC and SuperMUC-NG at
Leibniz Rechenzentrum (LRZ) in Garching,
Leonardo at CINECA in Bologna, Italy,
Supercomputing Wales, supported by the European Regional Development Fund via the Welsh Government,
the DiRAC Extreme Scaling and Data Intensive services, which are part of the UKRI Digital Research Infrastructure,
and
Frontera at TACC, TX, US\@.

\bibliographystyle{JHEP}
\bibliography{refs}

\end{document}
