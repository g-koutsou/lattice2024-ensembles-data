% Please make sure you insert your
% data according to the instructions in PoSauthmanual.pdf
\documentclass[a4paper,11pt]{article}
\usepackage{pos}
\usepackage{lineno}[pagewise]
\usepackage{forloop}
\usepackage[para]{footmisc}

\NewDocumentCommand\Nf{mggg}{$N_{\textrm{f}}\text{=}#1\IfNoValueTF{#2}{}{\text{+}#2}\IfNoValueTF{#3}{}{\text{+}#3}\IfNoValueTF{#4}{}{\text{+}#4}$}
\NewDocumentCommand\Nas{mggg}{$N_{\textrm{as}}\text{=}#1\IfNoValueTF{#2}{}{\text{+}#2}\IfNoValueTF{#3}{}{\text{+}#3}\IfNoValueTF{#4}{}{\text{+}#4}$}
\NewDocumentCommand\Nadj{mggg}{$N_{\textrm{adj}}\text{=}#1\IfNoValueTF{#2}{}{\text{+}#2}\IfNoValueTF{#3}{}{\text{+}#3}\IfNoValueTF{#4}{}{\text{+}#4}$}
\NewDocumentCommand\vol{mg}{#1\textsuperscript{3}\IfNoValueTF{#2}{}{$\times$#2}}

\newcounter{linecounter}
\newenvironment{emptylines}[1]{
  \begin{linenumbers}[1]
    \forloop{linecounter}{1}{\value{linecounter} < #1}{
      \quad\\
    }
  \end{linenumbers}
}{}

\title{Lattice gauge ensembles and data management}

\author[a,\#]{Yasumichi~Aoki}       \affiliation[a]{RIKEN Center for Computational Science (R-CCS), Kobe, 650-0047, Japan}
\author*[b,1]{Ed~Bennett}     \affiliation[b]{Swansea Academy of Advanced Computing, Swansea University, Swansea, United Kingdom}
\author*[c,2]{Rayan~Bignell}     \affiliation[c]{School of Mathematics, Trinity College, Dublin, Ireland}
\author*[d,3]{Kadir~Utku~Can}      \affiliation[d]{CSSM, Department of Physics, The University of Adelaide, Australia}
\author*[e,4]{Takumi~Doi}         \affiliation[e]{Interdisciplinary Theoretical and Mathematical Sciences Program (iTHEMS), RIKEN, Japan}
\author*[f,5]{Steven~Gottlieb}    \affiliation[f]{Department of Physics, Indiana University, IN, USA}
\author*[g,6]{Rajan~Gupta}       \affiliation[g]{Theoretical Division, Los Alamos National Laboratory, NM, USA}
\author*[h,7]{Georg~von Hippel}  \affiliation[h]{PRISMA+ Cluster of Excellence and Institut f\"ur Kernphysik}
\author*[a,8]{Issaku~Kanamori}   
\author*[i,9]{Andrey~Kotov}       \affiliation[i]{J\"ulich Supercomputing Center, Forschungszentrum J\"ulich, Germany}
\author*[j,10,\#]{Giannis~Koutsou} \affiliation[j]{Computation-based Science and Technology Research Center, The Cyprus Institute, Cyprus}
\author*[k,11]{Robert~Mawhinney}  \affiliation[k]{Physics Department, Columbia University, USA}
\author*[l,12]{Agostino~Patella}    \affiliation[l]{Humboldt Universit\"at zu Berlin, Institut f\"ur Physik \& IRIS, Germany}
\author*[i,13]{Giovanni Pederiva}   
\author*[m,14]{Christian Schmidt}    \affiliation[m]{Fakult\"at f\"ur Physik, Universit\"at Bielefeld, Germany}
\author*[n,15]{Takeshi Yamazaki}   \affiliation[n]{Institute of Pure and Applied Sciences, University of Tsukuba, Japan}
\author*[o,16]{Yi-Bo~Yang}    \affiliation[o]{CAS Key Laboratory of Theoretical Physics, Institute of Theoretical Physics, Chinese Academy of Sciences, Beijing 100190, China}

\note[1]{For the TELOS collaboration}
\note[2]{For the FASTSUM collaboration}
\note[3]{For the QCDSF collaboration}
\note[4]{For the HAL QCD collaboration}
\note[5]{For the MILC collaboration}
\note[6]{For the Jlab/W\&M/LANL/MIT/Marseille effort}
\note[7]{For the CLS}
\note[8]{For the JLQCD collaboration}
\note[9]{For the TWEXT collaboration}
\note[10]{For the ETM collaboration (ETMC)}
\note[11]{For the RBC-UKQCD collaboration}
\note[12]{For the RC* collaboration}
\note[13]{For the OPEN LAT initiative}
\note[14]{For the HotQCD collaboration}
\note[15]{For the PACS collaboration}
\note[16]{For the CLQCD collaboration}

\note[\#]{Conveners}

%\emailAdd{g.koutsou@cyi.ac.cy}

\abstract{We summarize the status of lattice QCD ensemble generation
  efforts and their data management characteristics. Namely, this
  proceeding summarizes contributions to a dedicated parallel session
  during the 41\textsuperscript{st} International Symposium on Lattice
  Field Theory (Lattice 2024), during which representatives of 16
  lattice QCD collaborations provided details on their simulation
  program, with focus on plans for publication, data management, and
  storage requirements. The parallel session was organized by the
  International Lattice Data Grid (ILDG), following an open call to
  the lattice QCD for participation in the session.}


\FullConference{The 41st International Symposium on Lattice Field
  Theory (LATTICE2024)\\ 28 July - 3 August 2024\\ Liverpool, UK\\}

\tableofcontents

\begin{document}
\maketitle

\section{Introduction}
The simulation of Quantum Chromodynamics (QCD) via its Euclidean-time,
discrete formulation on a lattice, has been one of the most
compute-intensive applications in scientific computing, consuming
substantial fractions of computer time at leadership HPC facilities
internationally. In particular, the generation of ensembles of gauge
configurations, for multiple values of the QCD parameters such as the
QCD coupling, the quark masses, and the extent of the finite volume,
requires multi-year simulation campaigns, coordinated by multi-member
research collaborations. It is thus common that collaborations store
and reuse the same gauge ensembles for multiple observables of
interest, and in many cases also share the ensembles with researchers
external to the collaboration that generated them.

The purpose of this proceeding is to summarize the available gauge
ensembles generated by various lattice QCD collaborations
internationally, with a focus on the data management practices each
collaboration employs. It follows a parallel session at the
41\textsuperscript{st} International Symposium on Lattice Field Theory
(Lattice 2024), during which 16 collaborations provided status reports
of their simulation efforts, responding to an open call for
participation addressed to the lattice QCD community prior to the
conference. The first such session was during Lattice 2022 and a
report of the contributions presented during that session can be found
in Ref.~\cite{Bali:2022mlg}.

These sessions are organized by the International Lattice Data Grid
(ILDG) with the intention of obtaining, gathering, and summarizing the
evolving needs of the lattice community in terms of data storage and
management.  The ILDG was setup in the early
2000s~\cite{Davies:2002mu,Yoshie:2008aw,Maynard:2009szr,Beckett:2009cb}
by the lattice community, which realized early on the value in
standardizing data management practices across the field. ILDG is
organized as a federation of autonomous \textit{regional grids},
within a single Virtual Organization~\cite{ildg-organization}. It
standardizes interfaces for the services, which are to be operated by
each regional grid, such as storage and a searchable metadata catalog,
so that the regional services are interoperable. Within ILDG, working
groups specify community-wide agreed metadata schemas
(QCDml)~\cite{Coddington:2007gz} to concisely mark-up the gauge
configurations and develop relevant middleware tools for facilitating
the use of ILDG services. The middleware and metadata specifications
developed by ILDG adhere to most of the FAIR (Findable, Accessible,
Interoperable, Reusable) principles~\cite{Wilkinson2016}. A summary of
recent developments in ILDG, referred to as ILDG 2.0, was presented
during the same session and can be found in a separate
proceeding~\cite{ILDG2}.

In the remainder of this proceeding, we present the status of ensemble
generation of each of the 16 collaborations that contributed to the
parallel session. We restrict ourselves to simulations of QCD, and at
present these are carried out using \Nf{2}{1}, \Nf{2}{1}{1}, and
\Nf{1}{1}{1}{1} sea quark flavors with various fermion
discretizations. The contributors were asked to specify whether their
data are public or if they plan on making them public, their interest
in using ILDG services and tools for that purpose, as well as some
overall information regarding storage requirements. This information
is collected in a table and summary section that follows the
individual contributions.

\section{Contributions}
The contributions from each collaboration follow, in the order
presented during the parallel session. The original presentations can
be found on the conference website~\cite{parallel-session}.

\newpage
\subsection{CLQCD}

The CLQCD collaboration focuses on the first principles QCD study of
the spectrum of exotic hadrons, parton structure of the nucleon, and
other traditional hadron, N-point correlation functions related to
high accuracy tests of the standard model, and also QCD in extreme
conditions. For this purpose, CLQCD generated a set of configurations
of \Nf{2}{1} fermions using the tadpole improved clover fermion action
with stout smearing and the tadpole improved Symanzik gauge action, at
5 values of the lattice spacing $\in[0.05,0.11]$~fm, several pion
masses $\in[120, 350]$~MeV, several volumes with
$m_{\pi}L\in[2.6,5.8]$~fm, and several temperatures
$\in(0,464]$~MeV. In addition, there are several anisotropic ensembles
  using clover fermions with $\xi=5$ at different lattice spacing,
  pion mass, volume and flavors which concentrate on the high
  precision studies related to glueball properties. For the gauge
  generation the Chroma package~\cite{Edwards:2004sx} with
  QUDA~\cite{Clark:2009wm,Babich:2011np,Clark:2016rdz} is used at
  present, and we plan to switch to PyQUDA~\cite{Jiang:2024lto} with
  QUDA in the near future. Currently the CLQCD collaboration has
  $\sim$20 ensembles (one ensemble corresponds to one point in the
  space lattice spacing-spatial volume-pion mass-temperature), which
  occupy $\sim 100$ TB of disk space. Configurations are stored in the
  SCIDAC format. Possible collaborations on the analysis of the
  correlation functions are welcome and CLQCD is preparing the
  hardware and software for the data sharing service.


\subsection{Jlab/W\&M/LANL/MIT/Marseille}
\emptylines{19}
\newpage
\subsection{HotQCD}
\emptylines{19}

\subsection{FASTSUM}
The FASTSUM collaboration use \Nf{2}{1} flavor anisotropic gauge ensembles in the fixed-scale approach to study the behavior of QCD as a function of temperature in hadronic and plasma phases. Specifically we have considered the behavior of hadronic states including light, strange, charm and bottom quarks, the electrical conductivity of QCD matter, the interquark potential and properties of the chiral transition. % The fixed-scale approach allows for a conceptually clear investigation of temperature effects while anisotropy allows more temporal points at each temperature.
FASTSUM gauge fields utilise an $\mathcal{O}\left(a^2\right)$ improved Symanzik gauge action and an $\mathcal{O}\left(a\right)$ improved spatially stout-smeared Wilson-Clover action following the parameter tuning and zero-temperature ensembles of the Hadron Spectrum collaboration~\cite{Edwards:2008ja,HadronSpectrum:2008xlg}. ``Generation 2'' ensembles were generated using the Chroma~\cite{Edwards:2004sx} software suite while the newer ``Generation 2L'' used a modfication~\cite{glesaaen_jonas_rylund_2018_2216355} of the {\sc openQCD}~\cite{Luscher:2012av,openqcd} package which introduces stout-link smearing and anisotropic actions. Generation 2 and 2L have an anisotropy $\xi = a_s/a_\tau \sim 3.5$ with $a_s\sim 0.12$ fm, $N_s = 24$ or $32$ and a wide range of $N_\tau$ corresponding to $T \in[44,760]$ MeV. %``Generation 2'' and ``Generation 2L'' differ mainly in their quark mass.
Full details of these ensembles may be found in Refs.~\cite{Aarts:2014nba,Aarts:2020vyb}. We are in the process of production for ``Generation 3'' - a parameter set similar to ``Generation 2'' but with twice the anistropy $\xi \sim 7$ - using {\sc openQCD-FASTSUM}~\cite{glesaaen_jonas_rylund_2018_2216355}. %As we use a derivative of the {\sc openQCD} package log, files include information such as the {\sc openQCD} version, algorithmic parameters, plaquette values and the run time. Additionally
We maintain a centralised metadata repository detailing (among other information) who was responsible for each run, on which machine that run was produced and where copies may be found. The gauge fields are redundantly stored on two (well-separated) storage servers managed by Swansea University in the openQCD format. The ``Generation 2'' ensembles are publically available~\cite{aarts_2024_8403827} while other ensembles will be available after an embargo period. We anticipate making ensembles available through the next incarnation of the ILDG with supplementary information also available on Zenodo~\cite{zenodo,aarts_2024_8403827}.
%\emptylines{19}

\subsection{TELOS}

The TELOS collaboration performs
\textbf{T}heoretical \textbf{E}xplorations on the \textbf{L}attice
with \textbf{O}rthogonal and \textbf{S}ymplectic groups.
Problems of interest focus on physics beyond the Standard Model,
in particular composite Higgs models.
Our work to date has made use of
the Wilson gauge action and Wilson fermion action.
Our ensembles include studies of
the Sp(4) theory with two fundamental fermion flavours
(\Nf{2})~\cite{Bennett:2019jzz}
(five values of $\beta\in[6.9,7.5]$,
$V \le 48\times42^{3}$,
$m_{\mathrm{PS}}/m_{\mathrm{V}} \gtrsim 0.407(16)$),
the Sp(4) theory with three antisymmetric fermion flavours
(\Nas{3})~\cite{Hsiao:2022gju},
(six values of $\beta\in[6.6,6.9]$,
$V \le 54\times36^{3}$,
$m_{\mathrm{ps}}/m_{\mathrm{v}} \gtrsim 0.7954(44)$);
and the Sp(4) theory with \Nf{2} and \Nas{3}~\cite{Bennett:2022yfa},
(three values of $\beta\in[6.45,6.5]$,
$V \le 56\times36^{3}$,
$m_{\mathrm{PS}}/m_{\mathrm{V}} \gtrsim 0.8768(30)$,
$m_{\mathrm{ps}}/m_{\mathrm{v}} \gtrsim 0.9022(27)$).
In the latter two cases,
the topological charge becomes slow running at small $m_{\mathrm{as}}$,
and at larger $\beta$.
We do not retain our pure gauge ensembles,
used for studies of the large-$N$ limit of Sp(2$N$)~\cite{Bennett:2023qwx,Bennett:2022gdz,Bennett:2020qtj},
since the costs of storage and data transfer are
higher than those of regenerating the ensembles.

Additionally,
we present ensembles generated by a subset of the collaboration,
with applications to conformal and near-conformal dynamics,
and to potential Walking Technicolor theories,
again using the Wilson gauge and Wilson fermion actions~\cite{Athenodorou:2024rba}.
Specifically,
these are SU(2) with one adjoint flavour (\Nadj{1})
(seven values of $\beta\in[2.05,2.4]$,
$V \le 96\times48^{3}$,
$m_{2^{+}_{s}} \gtrsim 0.28$),
and \Nadj{2}
($V \le 128\times64^{3}$,
$m_{2^{+}_{s}} \gtrsim 0.47$).
In the former case,
the majority of ensembles show ergodic topology,
with $\beta=2.4$ being marginal;
in the latter,
at large volumes we see significant topological freezing.

Ensembles are generated using
HiRep~\cite{Bennett:2019cxd,DelDebbio:2008zf}
and Grid~\cite{Bennett:2023gbe,Yamaguchi:2022feu}.
The above ensembles will be made available
as soon as the infrastructure is in place to do so.

We are in the process of generating ensembles
for Sp(4) \Nf{2}
and for SU(2) \Nf{1,2}
with Möbius domain wall fermions,
and for SU(2) \Nf{1,2}
with Wilson fermions
and additional Pauli--Villars fields,
which we aim to make available concurrently with the corresponding papers.



\subsection{HAL QCD}
\emptylines{19}

\subsection{TWEXT}
\begin{linenumbers}[1]
The TWEXT (Twisted Wilson @ Extreme conditions) collaboration studies
the properties of QCD at high temperature using Wilson Twisted Mass
fermions. Problems under investigation include chiral properties of
QCD, in particular the behavior of QCD around the chiral phase
transition and its scaling window~\cite{Kotov:2021rah}, topological
properties of QCD and QCD axion~\cite{Kotov:2021ujj}, hadron masses,
symmetries of QCD and others. For this purpose, TWEXT generated a set
of configurations for \Nf{2}{1}{1} fermions at the physical pion mass
and also uses older configurations with heavier pion
mass. Configurations with the physical pion mass have three lattice
spacings $a\in(0.057,0.080)$~fm and cover a wide range of temperatures
from $\sim120$ MeV to $\sim900$ MeV. It allows the TWEXT collaboration
to perform the continuum extrapolation for quantities of interest in
this temperature range. For the generation the tmLQCD software
package~\cite{Jansen:2009xp,Deuzeman:2013xaa,Abdel-Rehim:2013wba} is
used and the parameters of the ensembles were taken from the zero
temperature simulations of the ETM
collaboration~\cite{Alexandrou:2018egz}. Currently, the TWEXT
collaboration has 80 ensembles (one ensemble corresponds to one point
in the space temperature-pion mass-lattice spacing), which occupy
$\sim80$ TB of disk space. Configurations are stored in the ILDG
format. Possible collaborations are welcome and TWEXT plans to make
configurations public/use ILDG in the future, after performing the
ongoing analysis.
\end{linenumbers}

\subsection{QCDSF}
The main focus of the QCDSF collaboration is hadron spectrum and
structure at zero temperature.
%
%
Our ensembles are generated using the Symanzik improved gauge action
and Stout Link Non-perturbative Clover (SLiNC) fermion action, for
which the link variables appearing in the Dirac term are stout
smeared, while the links in the clover term are
not~\cite{Cundy:2009yy}. The clover coefficient is determined
non-perturbatively.
%
Our most recent set of ensembles are $2+1$-flavour, which cover pion
masses ranging $m_\pi^{phys} \lesssim m_\pi \lesssim 470 \; {\rm
  MeV}$, and 5 lattice spacings in between $a = 0.052 - 0.082 \; {\rm
  fm}$ (inclusive).
%
In total, there are 22 ensembles available and an additional 2 at
almost-physical pion mass still being generated. A recent listing of
available ensembles can be found in~\cite{QCDSFUKQCDCSSM:2023qlx}.
%
Our approach to the physical point follows the $\bar m^R = (2m_\ell^R
+ m_s^R)/3 = const$ line~\cite{Bietenholz:2011qq}, i.e. we start from
the SU(3) symmetric point where the renormalized masses of strange
$(m_s^R)$ and light quarks $(m_\ell^R)$ are equal to each other,
$m_s^R = m_\ell^R = \bar m^R/3$ and we increase $m_s^R$ as $m_\ell^R$
decreases.
%
%
The BQCD software suite~\cite{Haar:2017ubh} is used to generate the
ensembles, utilising the hybrid Monte Carlo (HMC) and rational HMC
algorithms.
%
%
All of our gauge configurations are stored in the ILDG format with
metadata compliant with the ILDG scheme. We have made use of ILDG
storage systems before, where the hub at the CSSM, Adelaide served as
one of the regional grids, and some older configurations are still
stored in the ILDG servers.
%
%
The QCDSF collaboration kindly asks any prospective users to contact
the collaboration before utilizing these configurations for their
projects. We plan to make newly generated ensembles available upon
request through ILDG, pending the collaboration's confirmation. QCDSF
is open to new collaborative projects.


\subsection{RBC-UKQCD}
\emptylines{19}

\subsection{OPEN LAT}
\emptylines{19}
\newpage
\subsection{RC*}
\emptylines{19}

\subsection{ETMC}
\begin{linenumbers}[1]
The ETM collaboration focuses on hadron spectroscopy, hadron
structure, and flavor physics at zero temperature. Ensembles employ
the twisted mass formulation, realizing $\mathcal{O}(a)$-improvement
by tuning to maximal twist, and include a clover term to further
reduce the size of lattice artifacts. The Iwasaki gauge action is
used. The main simulation effort is for the generation of ensembles
with degenerate up- and down-, strange- and charm-quarks
(\Nf{2}{1}{1}) with lattice spacing ranging between $0.049$ and
$0.091$~fm. $M_\pi\cdot L$ varies from $2.5$ up to ${\sim}5.5$. At the
time of writing, 24 ensembles are available or in the process of being
generated, with 8 of these at approximately physical values of the
quark masses. For a recent listing of the ensembles,
see~\cite{ETMCPoster:2024}. Simulations are performed using the Hybrid
Monte Carlo (HMC) algorithm implemented in the tmLQCD software
package~\cite{Jansen:2009xp,Deuzeman:2013xaa,Abdel-Rehim:2013wba}. See
Ref.~\cite{Alexandrou:2018egz} for details on the simulation program,
including the parameter tuning. The
DD-$\alpha$AMG~\cite{Frommer:2013fsa,Alexandrou:2016izb} multigrid
iterative solver is employed for the most poorly conditioned monomials
in the light sector while mixed-precision CG is used
elsewhere. Multi-shift CG is used together with shift-by-shift
refinement using DD-$\alpha$AMG~\cite{Alexandrou:2018wiv} for a number
of small shifts for the heavy sector. tmLQCD has interfaces to
QPhiX~\cite{Joo:2013lwm} and QUDA~\cite{Clark:2009wm,Babich:2011np}.
tmLQCD automatically writes gauge configurations in the ILDG format,
with meta-data including creation date, target simulation parameters,
and the plaquette. ETMC policy is to make ensembles publicly
available after a grace period. Older \Nf{2} and \Nf{2}{1}{1}
ensembles~\cite{Baron:2010bv,EuropeanTwistedMass:2010voq,ETM:2009ztk}
have made use of ILDG storage elements. The current ensembles are
available upon request and the collaboration intends to use ILDG in
the near future. For these ensembles, we expect storage requirements
to reach 3~PB.
\end{linenumbers}

\subsection{JLQCD}
\emptylines{19}

\subsection{MILC}
The MILC Collaboration has been creating ensembles of gauge
configurations for decades.  Initial calculations used two dynamical
flavors with naive staggered quarks.  Second generation efforts used
dynamical up, down, and strange quarks with the asqtad action,
culminating with a review in Ref.~\cite{MILC:2009mpl}.  Our third
generation calculations use the highly improved staggered quark (HISQ)
action \cite{Follana:2006rc}.  They contain dynamical up, down,
strange, and charm quarks.  For most of the ensembles, up and down are
degenerate.  The charm quark is set near its physical value.  For
several ensembles, the light quarks are near their average physical
value.  There are also ensembles with $m_l= 0.1,$ or $0.2 m_s$, where
$m_l$ ($m_s$) is the light (strange) quark mass.  We have generated a
number of ensembles with $m_s$ less than its physical value to explore
low energy constants and the chiral Lagrangian.  Lattice spacings are
in the range of $[0.15, 0.03]$ fm.  In a few cases, multiple volumes
are available.  Details about the generation of configurations can be
found in Refs.~\cite{MILC:2010pul,MILC:2012znn,Bazavov:2017lyh}.

The sharing policy for the MILC ensembles is available on
GitHub~\cite{MILC:policy}. On that web page can be found links to 1)
the sharing policy, 2) a Google Sheet detailing freely available
ensembles, and 3) a document summarizing which papers to cite for the
use of each ensemble.  Anyone wishing to use an ensemble that is not
listed in the Google Sheet, but has been used in a publication or
noted in a talk, is welcome to contact a member of the Fermilab
Lattice or MILC Collaborations to inquire as to whether the ensemble
can be made available for a specific project.  Many configurations are
available on USQCD resources, making access relatively easy for USQCD
members.  The ILDG is not operational in the US, but should it become
so, we would make an effort to use it.  We have assisted transfer of
configurations to other researchers both within and outside the US.

\subsection{CLS}
The CLS (Coordinated Lattice Simulations) effort uses the openQCD code
\cite{openqcd} to generate ensembles
\cite{Bruno:2014jqa,Mohler:2017wnb} with \Nf{2}{1} non-perturbatively
$\mathrm{O}(a)$-improved Wilson quarks and tree-level improved
Symanzik glue, mostly with open boundary conditions in time to avoid
topological freezing \cite{Schaefer:2010hu,Luscher:2012av}, but also
with (anti-)periodic boundary conditions in time (on some ensembles at
$a\gtrsim 0.06\,$fm), at six fine lattice spacings
$a\in[0.039,0.1]$\,fm and quark masses from the symmetric to the
physical point on three chiral trajectories
($\mathop{\mathrm{Tr}}[M]=\mathrm{const.}$,
$m_s\approx\mathrm{const.}$, $m_s=m_l$) in large volumes satisfying
$M_\pi L\ge4$ throughout, with statistics typically $\gtrsim 2,000$
MDU.

Due to the algorithm used, two reweighting factors are needed to use
CLS ensembles: one to correct for the twisted-mass stabilization of
the light quarks, and one to correct for the rational approximation to
$\sqrt{D^{\dagger}D}$ for the strange quark.  In the latter case,
$\det D<0$ can occur \cite{Mohler:2020txx}, so that one also needs to
correct for the wrong sign of the reweighting factor; fortunately, the
fraction of configurations with a negative reweighting factor is very
small (or zero) for most ensembles.

At the time of Lattice 2024, there were 149,766 configurations (1.384
PB) stored on tape in the openQCD (non-ILDG) data format.  Metadata
regarding data provenance, simulation setup, HMC stability, and data
integrity are collected automatically via automated scripts, while
reweighting factors and determinant minus signs are measured
separately.  A first batch of ensembles has been successfully uploaded
to ILDG; several more ensembles will follow soon, and the remainder
will follow after an embargo period.  The XML metadata are generated
automatically by extraction from the existing database; the (signed)
reweighting factors are included in the Config XML.

\subsection{PACS}
\emptylines{19}

\section{Summary}

\begin{table}[h]
  \caption{ Public: (2 = currently public, 1 = after an embargo period, 0 = no); ILDG: (N = no interest, I =
    interest, P = planned, U = already using); \#ens: Number of
    ensembles; \#cfg: Total number of configurations; storage: Total
    storage needed in TBytes.
    \label{tab:summary}
  }
  \centering
  \begin{tabular}{lccrrr}\hline\hline
    Collaboration                & Public & ILDG & \#ens & \#cfg   & Storage (TB) \\\hline
    CLQCD                        & 1      & I    & 20    & 10,000  & 100          \\
    Jlab/W\&M/LANL/MIT/Marseille &        &      &       &         &              \\
    HotQCD                       &        &      &       &         &              \\
    FASTSUM                      & 2,1    & I    & 37    & 37,000  & 65           \\
    TELOS                        & 1      & P    & 250   & 800,000 & 120          \\
    HAL QCD                      &        &      &       &         &              \\
    TWEXT                        & 1      & P    & 80    & 70,000  & 80           \\
    QCDSF                        & 1      & P,U  & 24    & 20,000  & 55           \\
    RBC-UKQCD                    &        &      &       &         &              \\
    OPEN LAT                     &        &      &       &         &              \\
    RC*                          &        &      &       &         &              \\
    ETMC                         & 1      & P,U  & 24    & 12,000  & 3,000        \\
    JLQCD                        &        &      &       &         &              \\
    MILC                         & 1      & I    & 50    & 70,000  & 600          \\
    CLS                          & 1      & P,U  & 25    & 150,000 & 1,400        \\
    PACS                         &        &      &       &         &              \\\hline\hline
  \end{tabular}
\end{table}


\acknowledgments G.~K. acknowledges support from EXCELLENCE/0421/0195,
co-financed by the European Regional Development Fund and the Republic
of Cyprus through the Research and Innovation Foundation and the
AQTIVATE a Marie Sk\l{}odowska-Curie Doctoral Network
GA~No.~101072344.  R.~B. acknowledges support from a Science
Foundation Ireland Frontiers for the Future Project award with grant
number SFI-21/FFP-P/10186. The work of E.~B. is part-funded by the
UKRI Science and Technology Facilities Council (STFC) Research
Software Engineer Fellowship EP/V052489/1, the UKRI Engineering and
Physical Science Research Council (EPSRC) ExCALIBUR programme ExaTEPP
project EP/X017168/1, and the STFC Consolidated Grant
ST/T000813/1. Y.-B.~Y. is supported in part by NSFC grants
No.~12293060, and~12435002. K.~U.~C. is supported by the Australian
Research Council grant DP190100297, DP220103098 and DP240102839


The participating collaborations acknowledge the following HPC systems
and HPC sites, for the generation of the gauge ensembles reported
here,\par LUMI-C and LUMI-G at CSC, Finland,\par Irène Joliot-Curie at
Très Grand Centre de Calcul (TGCC) in Bruyères-le-Châtel, France,\par
JUGENE, JUWELS and JUWELS-Booster at Jülich Supercomputing Centre
(JSC),\par HAWK at Höchstleistungsrechenzentrum Stuttgart (HLRS),
Germany,\par SuperMUC and SuperMUC-NG at Leibniz Rechenzentrum (LRZ)
in Garching, Germany,\par Kay and Stokes at the Irish Centre for
High-End Computing (ICHEC) in Galway, Ireland,\par Leonardo, Marconi
100 and Marconi A2 at CINECA in Bologna, Italy,\par Tesseract at the
DiRAC Extreme Scaling Service at the University of Edinburgh, United
Kingdom,\par DiRAC Data Intensive 2.5 \& 3 at the University of
Leicester, United Kingdom,\par Sunbird of Supercomputing Wales,
supported by the European Regional Development Fund via the Welsh
Government,\par the HPC Cluster of ITP-CAS, the Southern Nuclear
Science Computing Center (SNSC),\par Siyuan-1 cluster, supported by
the Center for High Performance Computing at Shanghai Jiao Tong
University, \par Frontera at TACC, TX, US,\par Cambridge Service for
Data Driven Discovery (CSD3),\par the North-German Supercomputer
Alliance (HLRN),\par the National Computer Infrastructure (NCI
National Facility in Canberra, Australia supported by the Australian
Commonwealth Government),\par the Pawsey Supercomputing Centre, which
is supported by the Australian Government and the Government of
Western Australia, \par and the DiRAC Extreme Scaling and Data
Intensive services, which are part of the UKRI Digital Research
Infrastructure.


\bibliographystyle{JHEP}
\bibliography{refs}

\end{document}
